# Proximal Policy Optimization Algorithms
## Abstract
alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent.
## Introduction
an algorithm that attains the data efficiency and reliable performance of TRPO, while using only first-order optimization. A novel objective with clipped probability ratios, which performs a pessimistic  estimate of the performance of the policy.
## Background: Policy Optimization
Policy Gradient Methods
Trust Region Methods
## Clipped Surrogate Objective  
## Adaptive KL Penalty Coefficient
## Algorithm
## Experiments